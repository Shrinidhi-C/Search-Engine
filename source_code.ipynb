{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AIR Assignment_36_code.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hsTvBquFhZI",
        "outputId": "1839dbbf-ed7e-4262-abfe-bd88adbaaa07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "from nltk.tokenize import sent_tokenize , word_tokenize\n",
        "import glob\n",
        "import re\n",
        "import csv\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "import sys\n",
        "Stopwords = set(stopwords.words('english'))\n",
        "porter = PorterStemmer()\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "!pip install textdistance\n",
        "!pip install elasticsearch\n",
        "from elasticsearch import Elasticsearch\n",
        "import textdistance\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "import time\n",
        "import ast\n",
        "from collections import Counter\n",
        "import copy \n",
        "import math"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "Collecting textdistance\n",
            "  Downloading https://files.pythonhosted.org/packages/35/71/87133323736b9b0180f600d477507318dae0abde613a54df33bfd0248614/textdistance-4.2.0-py3-none-any.whl\n",
            "Installing collected packages: textdistance\n",
            "Successfully installed textdistance-4.2.0\n",
            "Collecting elasticsearch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/ba/f950bdd9164fb2bbbe5093700162234fbe61f446fe2300a8993761c132ca/elasticsearch-7.10.0-py2.py3-none-any.whl (321kB)\n",
            "\u001b[K     |████████████████████████████████| 327kB 8.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3<2,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from elasticsearch) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from elasticsearch) (2020.6.20)\n",
            "Installing collected packages: elasticsearch\n",
            "Successfully installed elasticsearch-7.10.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1R-oVKaTxb9R"
      },
      "source": [
        "Creating Server for Elastic Search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OH28rmTyxajJ",
        "outputId": "dc17a380-6878-40a6-d27c-56816932179c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Downloading Elastic Search\n",
        "\n",
        "!apt install default-jdk > /dev/null\n",
        "!wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.5.4.tar.gz -q --show-progress\n",
        "!tar -xzf elasticsearch-6.5.4.tar.gz\n",
        "!chown -R daemon:daemon elasticsearch-6.5.4\n",
        "# start server\n",
        "import os\n",
        "from subprocess import Popen, PIPE, STDOUT\n",
        "es_server = Popen(['elasticsearch-6.5.4/bin/elasticsearch'], \n",
        "                  stdout=PIPE, stderr=STDOUT,\n",
        "                  preexec_fn=lambda: os.setuid(1)  # as daemon\n",
        "                 )\n",
        "# client-side\n",
        "!pip install elasticsearch -q\n",
        "from elasticsearch import Elasticsearch\n",
        "es = Elasticsearch()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "E: Failed to fetch http://security.ubuntu.com/ubuntu/pool/main/o/openjdk-lts/openjdk-11-jdk_11.0.9+11-0ubuntu1~18.04.1_amd64.deb  404  Not Found [IP: 91.189.88.152 80]\n",
            "E: Unable to fetch some archives, maybe run apt-get update or try with --fix-missing?\n",
            "elasticsearch-6.5.4 100%[===================>] 108.07M  32.2MB/s    in 3.4s    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbbwgNq6sr1b",
        "outputId": "7ac87be7-225e-4864-893e-9f2b3ca9cefe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KW4aBBTfOfIK"
      },
      "source": [
        "# function to convert nltk tag to wordnet tag\n",
        "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
        "    if nltk_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif nltk_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif nltk_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif nltk_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:          \n",
        "        return None\n",
        "\n",
        "def lemmatize_sentence(sentence):\n",
        "    #tokenize the sentence and find the POS tag for each token\n",
        "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n",
        "    #tuple of (token, wordnet_tag)\n",
        "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
        "    lemmatized_sentence = []\n",
        "    for word, tag in wordnet_tagged:\n",
        "        if tag is None:\n",
        "            #if there is no available tag, append the token as is\n",
        "            lemmatized_sentence.append(word)\n",
        "        else:        \n",
        "            #else use the tag to lemmatize the token\n",
        "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
        "    return \" \".join(lemmatized_sentence)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JilW1oenqZju"
      },
      "source": [
        "#Preprocessing\n",
        "\n",
        "def get_terms(line):\n",
        "    '''given a stream of text, get the terms from the text'''\n",
        "    line = line.lower()\n",
        "    line = re.sub(r'[^a-z0-9 ]', ' ', line)  # put spaces instead of non-alphanumeric characters\n",
        "    line = re.sub(' +', ' ', line)\n",
        "    words_tokens = word_tokenize(line)\n",
        "    # words_tokens = [word for word in words_tokens if len(words_tokens)>1]\n",
        "    line = [w for w in words_tokens if not w in Stopwords]\n",
        "    return line\n",
        "\n",
        "def preprocess(line):\n",
        "    line = re.sub(r\"(\\.|,|\\?|\\(|\\)|\\[|\\]|\\!|\\'|\\||\\%|\\:)\", \" \", line)\n",
        "    line = re.sub(r'[^a-z0-9 ]', ' ', line)  # put spaces instead of non-alphanumeric characters\n",
        "    line = re.sub('(?<=[a-z])\\'(?=[a-z])', '', line)\n",
        "    line = line.replace(\"-\", \"\")\n",
        "    line = line.replace(\"gonna\", \"going to\")\n",
        "    return line\n",
        "\n",
        "contractions = {\n",
        "\"ain't\": \"are not\",\n",
        "\"aren't\": \"am not\",\n",
        "\"can't\": \"cannot\",\n",
        "\"can't've\": \"cannot have\",\n",
        "\"'cause\": \"because\",\n",
        "\"could've\": \"could have\",\n",
        "\"couldn't\": \"could not\",\n",
        "\"couldn't've\": \"could not have\",\n",
        "\"didn't\": \"did not\",\n",
        "\"doesn't\": \"does not\",\n",
        "\"don't\": \"do not\",\n",
        "\"hadn't\": \"had not\",\n",
        "\"hadn't've\": \"had not have\",\n",
        "\"hasn't\": \"has not\",\n",
        "\"haven't\": \"have not\",\n",
        "\"he'd\": \"he would\",\n",
        "\"he'd've\": \"he would have\",\n",
        "\"he'll\": \"he will\",\n",
        "\"he'll've\": \"he will have\",\n",
        "\"he's\": \"he is\",\n",
        "\"how'd\": \"how did\",\n",
        "\"how'd'y\": \"how do you\",\n",
        "\"how'll\": \"how will\",\n",
        "\"how's\": \"how is\",\n",
        "\"i'd\": \"i had\",\n",
        "\"i'd've\": \"i would have\",\n",
        "\"i'll\": \"i will\",\n",
        "\"i'll've\": \"i will have\",\n",
        "\"i'm\": \"i am\",\n",
        "\"i've\": \"i have\",\n",
        "\"isn't\": \"is not\",\n",
        "\"it'd\": \"it would\",\n",
        "\"it'd've\": \"it would have\",\n",
        "\"it'll\": \"it will\",\n",
        "\"it'll've\": \"it will have\",\n",
        "\"it's\": \"it is\",\n",
        "\"let's\": \"let us\",\n",
        "\"ma'am\": \"madam\",\n",
        "\"mayn't\": \"may not\",\n",
        "\"might've\": \"might have\",\n",
        "\"mightn't\": \"might not\",\n",
        "\"mightn't've\": \"might not have\",\n",
        "\"must've\": \"must have\",\n",
        "\"mustn't\": \"must not\",\n",
        "\"mustn't've\": \"must not have\",\n",
        "\"needn't\": \"need not\",\n",
        "\"needn't've\": \"need not have\",\n",
        "\"o'clock\": \"of the clock\",\n",
        "\"oughtn't\": \"ought not\",\n",
        "\"oughtn't've\": \"ought not have\",\n",
        "\"shan't\": \"shall not\",\n",
        "\"sha'n't\": \"shall not\",\n",
        "\"shan't've\": \"shall not have\",\n",
        "\"she'd\": \"she would\",\n",
        "\"she'd've\": \"she would have\",\n",
        "\"she'll\": \"she will\",\n",
        "\"she'll've\": \"she will have\",\n",
        "\"she's\": \"she is\",\n",
        "\"should've\": \"should have\",\n",
        "\"shouldn't\": \"should not\",\n",
        "\"shouldn't've\": \"should not have\",\n",
        "\"so've\": \"so have\",\n",
        "\"so's\": \"so is\",\n",
        "\"that'd\": \"that had\",\n",
        "\"that'd've\": \"that would have\",\n",
        "\"that's\": \"that is\",\n",
        "\"there'd\": \"there had\",\n",
        "\"there'd've\": \"there would have\",\n",
        "\"there's\": \"there is\",\n",
        "\"they'd\": \"they had\",\n",
        "\"they'd've\": \"they would have\",\n",
        "\"they'll\": \"they will\",\n",
        "\"they'll've\": \"they will have\",\n",
        "\"they're\": \"they are\",\n",
        "\"they've\": \"they have\",\n",
        "\"to've\": \"to have\",\n",
        "\"wasn't\": \"was not\",\n",
        "\"we'd\": \"we would\",\n",
        "\"we'd've\": \"we would have\",\n",
        "\"we'll\": \"we will\",\n",
        "\"we'll've\": \"we will have\",\n",
        "\"we're\": \"we are\",\n",
        "\"we've\": \"we have\",\n",
        "\"weren't\": \"were not\",\n",
        "\"what'll\": \"what will\",\n",
        "\"what'll've\": \"what will have\",\n",
        "\"what're\": \"what are\",\n",
        "\"what's\": \"what is\",\n",
        "\"what've\": \"what have\",\n",
        "\"when's\": \"when is\",\n",
        "\"when've\": \"when have\",\n",
        "\"where'd\": \"where did\",\n",
        "\"where's\": \"where is\",\n",
        "\"where've\": \"where have\",\n",
        "\"who'll\": \"who will\",\n",
        "\"who'll've\": \"who will have\",\n",
        "\"who's\": \"who is\",\n",
        "\"who've\": \"who have\",\n",
        "\"why's\": \"why is\",\n",
        "\"why've\": \"why have\",\n",
        "\"will've\": \"will have\",\n",
        "\"won't\": \"will not\",\n",
        "\"won't've\": \"will not have\",\n",
        "\"would've\": \"would have\",\n",
        "\"wouldn't\": \"would not\",\n",
        "\"wouldn't've\": \"would not have\",\n",
        "\"y'all\": \"you all\",\n",
        "\"y'all'd\": \"you all would\",\n",
        "\"y'all'd've\": \"you all would have\",\n",
        "\"y'all're\": \"you all are\",\n",
        "\"y'all've\": \"you all have\",\n",
        "\"you'd\": \"you would\",\n",
        "\"you'd've\": \"you would have\",\n",
        "\"you'll\": \"you will\",\n",
        "\"you'll've\": \"you will have\",\n",
        "\"you're\": \"you are\",\n",
        "\"you've\": \"you have\",\n",
        "\"I'am\":\"I am\"\n",
        "}"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzfqN3fbpURq"
      },
      "source": [
        "#path='/content/drive/My Drive/TelevisionNews'\n",
        "path='/content/drive/My Drive/AIR/TelevisionNews'\n",
        "all_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
        "#print(len(all_files))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6ut8lSF8dQZ"
      },
      "source": [
        "#B-TREE implementation\n",
        "\n",
        "keywords=[]\n",
        "class Node:\n",
        "    def __init__(self, data):\n",
        "\n",
        "        self.left = None\n",
        "        self.right = None\n",
        "        self.data = data\n",
        "    def insert(self, data):\n",
        "# Compare the new value with the parent node\n",
        "        if self.data:\n",
        "            if data < self.data:\n",
        "                if self.left is None:\n",
        "                    self.left = Node(data)\n",
        "                else:\n",
        "                    self.left.insert(data)\n",
        "            elif data > self.data:\n",
        "                if self.right is None:\n",
        "                    self.right = Node(data)\n",
        "                else:\n",
        "                    self.right.insert(data)\n",
        "        else:\n",
        "            self.data = data\n",
        "\n",
        "    def PrintTree(self):\n",
        "        if self.left:\n",
        "            self.left.PrintTree()\n",
        "        keywords.append(self.data)\n",
        "        if self.right:\n",
        "            self.right.PrintTree()\n",
        "\n",
        "    def findval(self, lkpval):\n",
        "        if lkpval < self.data:\n",
        "            if self.left is None:\n",
        "                return \"0\"\n",
        "            return self.left.findval(lkpval) \n",
        "        elif lkpval > self.data:\n",
        "            if self.right is None:\n",
        "                return \"0\"\n",
        "            return self.right.findval(lkpval)\n",
        "        else:\n",
        "            return self.data"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEibNfRRlN0-"
      },
      "source": [
        "#creation of inverted index \n",
        "inverted_index={}\n",
        "tf={} #term Frequency\n",
        "doc_count=0\n",
        "doc_to_id={}\n",
        "root=None\n",
        "for file_no, file in enumerate(all_files):\n",
        "    try:\n",
        "      data_file= pd.read_csv(file)\n",
        "      df = pd.DataFrame(data_file)\n",
        "      df['Snippet'].dropna(inplace=True)\n",
        "      data=df[\"Snippet\"]\n",
        "      tf[file]={}\n",
        "      doc_to_id[file_no]=file\n",
        "      for snippet_no, line in enumerate(data):\n",
        "          doc_count+=1\n",
        "          line=preprocess(line)\n",
        "          for contraction in contractions:\n",
        "                if contraction in line:\n",
        "                    line=line.replace(contraction,contractions[contraction])\n",
        "          line=lemmatizer.lemmatize(line)\n",
        "          line_ref= word_tokenize(line)\n",
        "          tf[file][snippet_no]={}\n",
        "          terms=get_terms(line)\n",
        "          words = line.split(' ')\n",
        "          for term in terms:\n",
        "              if term in words:\n",
        "                  if not tf[file][snippet_no].get(term):\n",
        "                    tf[file][snippet_no][term]=1\n",
        "                  else:\n",
        "                    tf[file][snippet_no][term]+=1\n",
        "                  term_pos_list = [index for index, element in enumerate(terms) if element == term]\n",
        "\n",
        "                  if term not in inverted_index.keys():\n",
        "                      inverted_index[term]={}\n",
        "                      inverted_index[term][file_no]={}\n",
        "                      inverted_index[term][file_no][snippet_no]=term_pos_list\n",
        "                  else:\n",
        "                      try:\n",
        "                          inverted_index[term][file_no][snippet_no]=term_pos_list\n",
        "                      except:\n",
        "                          inverted_index[term][file_no]={}\n",
        "                          inverted_index[term][file_no][snippet_no]=term_pos_list\n",
        "\n",
        "                  if root==None:\n",
        "                      root = Node(term)\n",
        "                  else:\n",
        "                      root.insert(term)\n",
        "    except:\n",
        "      continue\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98akGAt6lNXi"
      },
      "source": [
        "#store the inverted index\n",
        "with open(r\"/content/drive/My Drive/AIR/res.txt\",'w') as f:\n",
        "    print(inverted_index, file=f)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0pYdz2n8_4z"
      },
      "source": [
        "file = open(r\"/content/drive/My Drive/AIR/res.txt\", \"r\")\n",
        "\n",
        "contents = file.read()\n",
        "inverted_index = ast.literal_eval(contents)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpyzmUniwX1c"
      },
      "source": [
        "#Document Frequency\n",
        "df={}\n",
        "for doc in tf.keys():\n",
        "  for snippet in tf[doc].keys():\n",
        "    for term in tf[doc][snippet]:\n",
        "      if df.get(term):\n",
        "        df[term]+=1\n",
        "      elif not df.get(term):\n",
        "        df[term]=1\n",
        "tf1 = copy.deepcopy(tf)\n",
        "for doc in tf1.keys():\n",
        "  for snippet in tf1[doc].keys():\n",
        "    for term in tf1[doc][snippet]:\n",
        "      l=len(tf1[doc][snippet])\n",
        "      tf1[doc][snippet][term]=tf[doc][snippet][term]/l"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPMetLoxQobt"
      },
      "source": [
        "#SPELL CHECK!!!\n",
        "df1=Counter(df)\n",
        "V=df1.keys()\n",
        "probs = {}   \n",
        "Total = max(df1.values()) \n",
        "for k in df1.keys():\n",
        "    probs[k] = df1[k]/Total\n",
        "\n",
        "def spell_check(input_word):\n",
        "    input_word = input_word.lower()\n",
        "    if input_word in V:\n",
        "        return('Your word seems to be correct')\n",
        "    else:\n",
        "        similarities = [1-(textdistance.Jaccard(qval=2).distance(v,input_word)) for v in df1.keys()]\n",
        "        df = pd.DataFrame(list(probs.items()),columns = ['word', 'Prob']) \n",
        "        df['Similarity'] = similarities\n",
        "        output = df.sort_values(['Similarity', 'Prob'], ascending=False).head(3)\n",
        "        return output"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuR8UGTwSlz3"
      },
      "source": [
        "#wild card \n",
        "def isMatch(s, p):\n",
        "      sl = len(s)\n",
        "      pl = len(p)\n",
        "      dp = [[False for i in range(pl+1)] for j in range(sl+1)]\n",
        "      s = \" \"+s\n",
        "      p = \" \"+p\n",
        "      dp[0][0]=True\n",
        "      for i in range(1,pl+1):\n",
        "         if p[i] == '*':\n",
        "            dp[0][i] = dp[0][i-1]\n",
        "      for i in range(1,sl+1):\n",
        "         for j in range(1,pl+1):\n",
        "            if s[i] == p[j] or p[j] == '?':\n",
        "               dp[i][j] = dp[i-1][j-1]\n",
        "            elif p[j]=='*':\n",
        "               dp[i][j] = max(dp[i-1][j],dp[i][j-1])\n",
        "      return dp[sl][pl]\n",
        "def wild_query(query):\n",
        "    result=[]\n",
        "    keywords=[]\n",
        "    if ('*' in query):\n",
        "        matched=[]\n",
        "        for word in df1.keys():\n",
        "            b=isMatch(str(word),query)\n",
        "            if (b==True):\n",
        "                keywords.append(word)\n",
        "            else:\n",
        "                continue\n",
        "    for keyword in (keywords):\n",
        "        res_term= root.findval(keyword)\n",
        "        result.append(res_term)\n",
        "    return result"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsHQnm1lzpds"
      },
      "source": [
        "Free text and Phrase QUery"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzuK4zO3vHw2"
      },
      "source": [
        "def intersect(p1,p2):\n",
        "    ans=[]\n",
        "    p1len=len(p1)\n",
        "    p2len=len(p2)\n",
        "    i=0\n",
        "    j=0\n",
        "    while i!=p1len and j!=p2len:\n",
        "        if p1[i]==p2[j]:\n",
        "            ans.append(p1[i])\n",
        "            i+=1\n",
        "            j+=1\n",
        "        else:\n",
        "            if p1[i]<p2[j]:\n",
        "                i+=1\n",
        "            else:\n",
        "                j+=1\n",
        "    return ans\n",
        "\n",
        "def intersect_prox(p1,p2,k):\n",
        "    l=[]\n",
        "    p1len=len(p1)\n",
        "    p2len=len(p2)\n",
        "\n",
        "    i=j=0\n",
        "    while i!=p1len:\n",
        "        while j!=p2len:\n",
        "            if abs(p1[i]-p2[j])<=k:\n",
        "                l.append(p2[j])\n",
        "            elif p2[j]>p1[i]:\n",
        "                break\n",
        "            j+=1\n",
        "        l.sort()\n",
        "        while l!=[] and abs(l[0]-p1[i])>k:\n",
        "            l.remove(l[0])\n",
        "        i+=1\n",
        "  \n",
        "    return l\n",
        "\n",
        "def sort_docs_list(docs_list):\n",
        "    docs_list.sort(key=len)\n",
        "    return docs_list\n",
        "\n",
        "\n",
        "def extract_docs(term):\n",
        "    docs_list=[]\n",
        "    for i in inverted_index[term].keys():\n",
        "        docs_list.append(i)\n",
        "    return docs_list\n",
        "\n",
        "def extract_snippets(term,doc_no):\n",
        "    snippets_list=[]\n",
        "    for i in inverted_index[term][doc_no].keys():\n",
        "        snippets_list.append(i)\n",
        "    return snippets_list\n",
        "\n",
        "\n",
        "def extract_positional_indices(term,doc_no,snip_no):\n",
        "    positions_list=[]\n",
        "    for i in inverted_index[term][doc_no][snip_no]:\n",
        "        positions_list.append(i)\n",
        "    return positions_list\n",
        "\n",
        "def phrase_proximity_freetext_query(query,inverted_index):\n",
        "    results=[]\n",
        "    if '/' in query:\n",
        "        query_new = re.sub(r'[0-9]+', '', query)\n",
        "        query_new = preprocess(query_new)\n",
        "    else:\n",
        "        query_new = preprocess(query)\n",
        "    \n",
        "    terms = get_terms(query_new)\n",
        "    words = terms\n",
        "    exact_words=  word_tokenize(query_new)\n",
        "\n",
        "    total_docs={}\n",
        "    for keyword in range(len(words)):\n",
        "        res_term= root.findval(words[keyword])\n",
        "        total_docs[res_term]=extract_docs(res_term)\n",
        "\n",
        "    \n",
        "    total_docs_list= list(total_docs.values())\n",
        "    total_docs_list= sort_docs_list(total_docs_list)\n",
        "    l=total_docs_list[0]\n",
        "    for i in total_docs_list:\n",
        "        l=intersect(l,i)\n",
        "\n",
        "    if l:\n",
        "        for doc in l:\n",
        "            total_snips={}\n",
        "            for term in list(total_docs.keys()):\n",
        "                total_snips[term]=extract_snippets(term,doc)\n",
        "\n",
        "            total_snips_list = list(total_snips.values())\n",
        "         \n",
        "            total_snips_list= sort_docs_list(total_snips_list)\n",
        "            s=total_snips_list[0]\n",
        "            for i in total_snips_list:\n",
        "                s=intersect(s,i)\n",
        "\n",
        "            if s:\n",
        "                for snip in s:\n",
        "                    total_positions={}\n",
        "                    for term in list(total_docs.keys()):\n",
        "                        total_positions[term]=extract_positional_indices(term,doc,snip)\n",
        "\n",
        "                    if '/' not in query and  \"\\\"\" in query:\n",
        "                        # print(\"Query Type: Phrase Query\")\n",
        "\n",
        "                        if len(exact_words)==1:\n",
        "                            # print('doc:snip:positions:',doc,snip,total_positions[exact_words[0]])\n",
        "                            # print(\"Phrase Query: found in document {}, snippet {}\".format(doc,snip))\n",
        "                            results.append([doc,snip])\n",
        "\n",
        "                        for i in range(len(exact_words)-1):\n",
        "                            p1= total_positions[exact_words[i]]\n",
        "                            p2= total_positions[exact_words[i+1]]\n",
        "                      \n",
        "                            for i in p1:  \n",
        "                                if i+1 in p2:\n",
        "                                    results.append([doc,snip])\n",
        "                                    # print(\"Phrase Query: found in document {}, snippet {}\".format(doc,snip))\n",
        "                                else:\n",
        "                                    pass\n",
        "                                    # print('no results')\n",
        "\n",
        "                    elif '/' in query :\n",
        "                        # print(\"Query Type: Proximity Query\")\n",
        "\n",
        "                        proximity = re.findall(r'\\d+' , query)\n",
        "                        query1 = query.split(\" \")\n",
        "                        query_new1= query_new.split(\" \")\n",
        "                        token = []\n",
        "                        terms=[]\n",
        "                        for i in query_new1:\n",
        "                            if '/' not in i and i!='':\n",
        "                                terms.append(i)\n",
        "                        token.append(query1[0])\n",
        "                   \n",
        "                        for i in range(0,len(query1)):\n",
        "                         \n",
        "                            if (\"/\" in query1[i]):\n",
        "                                k = int(proximity[0])\n",
        "                                p1=total_positions[token[0]]\n",
        "                                p2=total_positions[token[1]]\n",
        "                            \n",
        "                                documents = intersect_prox(p1,p2,k)\n",
        "                                if documents:\n",
        "                                    results.append([doc,snip])\n",
        "\n",
        "                                token.remove(token[0])\n",
        "                                proximity.remove(proximity[0])\n",
        "                            else:\n",
        "                               \n",
        "                                if i<len(terms)-1:\n",
        "                                    term = terms[i+1]\n",
        "                                elif i==len(terms)-1:\n",
        "                                    term = terms[i]\n",
        "\n",
        "                                token.append(term)\n",
        "\n",
        "                            if (len(token) == 3):\n",
        "                                token.remove(token[0])\n",
        "\n",
        "                    elif  '/'  not in query and  \"\\\"\" not  in query:\n",
        "                         print(\"Query Type: Free Text Query\")\n",
        "                         print(\" free Query: found in document {},  snippet {}\".format(doc,snip))\n",
        "    return results\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5k_QSbxXdlz",
        "outputId": "de1a5a76-69e7-4d7c-8480-1fc49784c04c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        }
      },
      "source": [
        "### AND OPEARTION ON QUERY\n",
        "'''\n",
        "res={}\n",
        "min_word=''\n",
        "min_count=100000\n",
        "for keyword in keywords:\n",
        "  c=0\n",
        "  for doc in inverted_index[keyword].keys():\n",
        "    c+=len(inverted_index[keyword][doc].keys())\n",
        "  if(min_count>c):\n",
        "    min_word=keyword\n",
        "    min_count=c\n",
        "for doc in inverted_index[min_word].keys():\n",
        "  for snippet in inverted_index[min_word][doc].keys():\n",
        "    same=1\n",
        "    for keyword in keywords:\n",
        "      if(doc in inverted_index[keyword].keys()) and (snippet in inverted_index[keyword][doc].keys()):\n",
        "        continue\n",
        "      else:\n",
        "        same=0\n",
        "        break\n",
        "    if(same==1):\n",
        "      if(doc in res.keys()):\n",
        "        res[doc].append(snippet)\n",
        "      else:\n",
        "        res[doc]=[]\n",
        "        res[doc].append(snippet)'''"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nres={}\\nmin_word=''\\nmin_count=100000\\nfor keyword in keywords:\\n  c=0\\n  for doc in inverted_index[keyword].keys():\\n    c+=len(inverted_index[keyword][doc].keys())\\n  if(min_count>c):\\n    min_word=keyword\\n    min_count=c\\nfor doc in inverted_index[min_word].keys():\\n  for snippet in inverted_index[min_word][doc].keys():\\n    same=1\\n    for keyword in keywords:\\n      if(doc in inverted_index[keyword].keys()) and (snippet in inverted_index[keyword][doc].keys()):\\n        continue\\n      else:\\n        same=0\\n        break\\n    if(same==1):\\n      if(doc in res.keys()):\\n        res[doc].append(snippet)\\n      else:\\n        res[doc]=[]\\n        res[doc].append(snippet)\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iiQVu6IavnD"
      },
      "source": [
        "### OR OPEARTION ON QUERY\n",
        "def OR(keywords):\n",
        "  res={}\n",
        "  for keyword in keywords:\n",
        "    for doc in inverted_index[keyword].keys():\n",
        "      if(doc not in res):\n",
        "        res[doc]=[]\n",
        "      res[doc]=list(set(res[doc]+list(inverted_index[keyword][doc].keys())))\n",
        "  return res"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "204xQ3DTb1PT"
      },
      "source": [
        "#TF-IDF vector\n",
        "def generateVectors(keywords, res,ret_count):\n",
        "    tf_idf_matrix = np.zeros((len(keywords), ret_count))\n",
        "    for i, s in enumerate(keywords):\n",
        "        idf = math.log(doc_count/df[s])+1\n",
        "        j=0\n",
        "        for doc in res.keys():\n",
        "          for snippet in res[doc]:\n",
        "            try:\n",
        "              tf_idf_matrix[i][j] = idf * tf1[doc_to_id[doc]][snippet][s]\n",
        "            except:\n",
        "              tf_idf_matrix[i][j] =0\n",
        "            j+=1\n",
        "    return tf_idf_matrix"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgm-6n7JLpml"
      },
      "source": [
        "#Vector Query\n",
        "\n",
        "def wcount(s):\n",
        "    counts = dict()\n",
        "    words = s\n",
        "    for word in words:\n",
        "        if word in counts:\n",
        "            counts[word] += 1\n",
        "        else:\n",
        "            counts[word] = 1\n",
        "    return counts\n",
        "\n",
        "\n",
        "def build_query_vector(keywords, res):\n",
        "    count = wcount(keywords)\n",
        "    vector = np.zeros((len(count),1))\n",
        "    for i, word in enumerate(keywords):\n",
        "          vector[i] = float(count[word])/len(count) * (1+math.log(doc_count/df[word]))\n",
        "    return vector\n",
        "    \n"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_-qIF4ZWSCL"
      },
      "source": [
        "def consine_similarity(v1, v2):\n",
        "    return np.dot(v1,v2)/float(np.linalg.norm(v1)*np.linalg.norm(v2))\n",
        "def compute_relevance(query, documents,tf_idf_matrix,query_vector,res):\n",
        "    results=[]\n",
        "    c=0\n",
        "    for doc in res.keys():\n",
        "      for snippet in res[doc]:\n",
        "        similarity = consine_similarity(tf_idf_matrix[:,c].reshape(1, len(tf_idf_matrix)), query_vector)\n",
        "        l=[doc_to_id[doc],snippet, float(similarity[0])]\n",
        "        results.append(l)\n",
        "        #print(\"document {}, snippet {}, similarity {}\".format(doc_to_id[doc],snippet, float(similarity[0])))\n",
        "        c+=1\n",
        "    return results\n",
        "        \n"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKUhYvrvyNLY"
      },
      "source": [
        " **USER** **SEARCH**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUgiYoqXu2UH"
      },
      "source": [
        "def query_func():\n",
        "  query=\"\"\n",
        "  while query==\"\":\n",
        "    query= input('Enter the query:')\n",
        "    global start_time\n",
        "    start_time=time.time()\n",
        "  if '*' in query:\n",
        "      keywords=wild_query(query)\n",
        "  else:\n",
        "      keywords=phrase_proximity_freetext_query(query,inverted_index)\n",
        "  return keywords\n",
        "\n",
        "# if in \" \": phrase query\n",
        "# if /k present: proximity query\n",
        "# if \"*\" present: wildcard query\n",
        "# if free text: (OR) of all the terms"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usUMZumgugvm"
      },
      "source": [
        ""
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3Fqkn5k7HpP",
        "outputId": "50e9a477-e325-4aee-c8ac-a16c6090473b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "results=[]\n",
        "start_time=0\n",
        "def main():\n",
        "  keywords=[]\n",
        "  keywords=query_func()\n",
        "  print(keywords)\n",
        "  try:\n",
        "    res=OR(keywords)\n",
        "    ret_count=0\n",
        "    for doc in res.keys():\n",
        "      ret_count+=len(res[doc])  \n",
        "    tf_idf_matrix = generateVectors(keywords, res,ret_count)\n",
        "    query_vector = build_query_vector(keywords, res)\n",
        "    results=compute_relevance(words,res,tf_idf_matrix,query_vector,res)\n",
        "    results=sorted(results,key=lambda l:l[2], reverse=True)\n",
        "    c=1\n",
        "    print(\"Response_time: \",time.time()-start_time)\n",
        "    for res in results:\n",
        "      if(c==11):\n",
        "        break\n",
        "      print(\"Document :\",c)\n",
        "      c+=1\n",
        "      df=pd.read_csv(res[0])\n",
        "      print(\"Similarity : \",res[2])\n",
        "      for col in df.columns:\n",
        "        print(col,\" : \",df[col][res[1]])\n",
        "  except:\n",
        "\n",
        "    print(\"Response_time: \",time.time()-start_time)\n",
        "    for res in keywords:\n",
        "      doc= doc_to_id[res[0]]\n",
        "      print(\"Document :\",doc)\n",
        "      df = pd.read_csv(doc)\n",
        "    \n",
        "      for col in df.columns:\n",
        "        print(col,\" : \",df[col][res[1]])\n",
        "\n",
        "\n",
        "\n",
        "main()"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter the query:\"google\"\n",
            "[[4, 49], [25, 0], [31, 47], [32, 39], [49, 83], [59, 28], [59, 885], [59, 996], [59, 1043], [59, 1259], [69, 933], [69, 936], [76, 843], [76, 1446], [79, 632], [80, 202], [80, 992], [80, 1090], [96, 13], [145, 103], [146, 13], [146, 14], [166, 17], [170, 327], [170, 328], [170, 329], [170, 330], [170, 361], [170, 362], [170, 442], [170, 443], [172, 84], [195, 50], [195, 51], [212, 45], [250, 39], [258, 51], [258, 75], [258, 99], [276, 13], [284, 193], [284, 205], [287, 12], [287, 172], [303, 5], [303, 6], [303, 29], [305, 28], [323, 210], [333, 263], [333, 290], [333, 291], [333, 303], [338, 143], [338, 213], [351, 186], [366, 248], [374, 149], [384, 140], [388, 115], [394, 12], [394, 18], [415, 704]]\n",
            "Response_time:  0.0013194084167480469\n",
            "Document : /content/drive/My Drive/AIR/TelevisionNews/CNN.201004.csv\n",
            "URL  :  https://archive.org/details/CNN_20100429_170000_CNN_Newsroom#start/1100/end/1135\n",
            "MatchDateTime  :  4/29/2010 17:18:35\n",
            "Station  :  CNN\n",
            "Show  :  CNN Newsroom\n",
            "IAShowID  :  CNN_20100429_170000_CNN_Newsroom\n",
            "IAPreviewThumb  :  https://archive.org/download/CNN_20100429_170000_CNN_Newsroom/CNN_20100429_170000_CNN_Newsroom.thumbs/CNN_20100429_170000_CNN_Newsroom_000990.jpg\n",
            "Snippet  :  where it won't get into the atmosphere. exxonmobil is spending more than 100 million dollars. to build a plant that will demonstrate this process. i'm very optimistic about it. because this technology could be used. to reduce greenhouse gas emissions significantly. okay, blackberries, google,\n",
            "Document : /content/drive/My Drive/AIR/TelevisionNews/BBCNEWS.201708.csv\n",
            "URL  :  https://archive.org/details/BBCNEWS_20170805_180000_BBC_News#start/927/end/962\n",
            "MatchDateTime  :  8/5/2017 18:15:42\n",
            "Station  :  BBCNEWS\n",
            "Show  :  BBC News\n",
            "IAShowID  :  BBCNEWS_20170805_180000_BBC_News\n",
            "IAPreviewThumb  :  https://archive.org/download/BBCNEWS_20170805_180000_BBC_News/BBCNEWS_20170805_180000_BBC_News.thumbs/BBCNEWS_20170805_180000_BBC_News_000898.jpg\n",
            "Snippet  :  google warming is now inevitable from the mind of greenhouse gases put into the atmosphere. we have to learn to adapt, protect ourselves, improved buildings and shading and so on. so on. air conditioning is quite effective but is also costly of energy itself and contribute more greenhouse gas emissions. people\n",
            "Document : /content/drive/My Drive/AIR/TelevisionNews/BBCNEWS.201802.csv\n",
            "URL  :  https://archive.org/details/BBCNEWS_20180216_140000_Afternoon_Live#start/8092/end/8127\n",
            "MatchDateTime  :  2/16/2018 16:15:07\n",
            "Station  :  BBCNEWS\n",
            "Show  :  Afternoon Live\n",
            "IAShowID  :  BBCNEWS_20180216_140000_Afternoon_Live\n",
            "IAPreviewThumb  :  https://archive.org/download/BBCNEWS_20180216_140000_Afternoon_Live/BBCNEWS_20180216_140000_Afternoon_Live.thumbs/BBCNEWS_20180216_140000_Afternoon_Live_008067.jpg\n",
            "Snippet  :  forces supporting the iraqi government to territory in their brave fight against daesh in the middle east. and in areas like google health, climate change, uk germany corporation has shaped the agenda internationally. in our talks\n",
            "Document : /content/drive/My Drive/AIR/TelevisionNews/CNN.201106.csv\n",
            "URL  :  https://archive.org/details/CNNW_20110613_150000_CNN_Newsroom#start/517/end/552\n",
            "MatchDateTime  :  6/13/2011 15:08:52\n",
            "Station  :  CNN\n",
            "Show  :  CNN Newsroom\n",
            "IAShowID  :  CNNW_20110613_150000_CNN_Newsroom\n",
            "IAPreviewThumb  :  https://archive.org/download/CNNW_20110613_150000_CNN_Newsroom/CNNW_20110613_150000_CNN_Newsroom.thumbs/CNNW_20110613_150000_CNN_Newsroom_000510.jpg\n",
            "Snippet  :  run, and why romney was for a health care mandate. and tim pawlenty will have to own up to why he once believed in fighting climate change and notice does not. he'll also be likely asked about the google litmus test, you\n",
            "Document : /content/drive/My Drive/AIR/TelevisionNews/BBCNEWS.201808.csv\n",
            "URL  :  https://archive.org/details/BBCNEWS_20180829_043000_Business_Briefing#start/859/end/894\n",
            "MatchDateTime  :  8/29/2018 4:44:34\n",
            "Station  :  BBCNEWS\n",
            "Show  :  Business Briefing\n",
            "IAShowID  :  BBCNEWS_20180829_043000_Business_Briefing\n",
            "IAPreviewThumb  :  https://archive.org/download/BBCNEWS_20180829_043000_Business_Briefing/BBCNEWS_20180829_043000_Business_Briefing.thumbs/BBCNEWS_20180829_043000_Business_Briefing_000838.jpg\n",
            "Snippet  :  warned social media giants google, facebook and twitter are 'treading on troubled territory', amid a row about perceived bias. he's accused google of rigging results for the search phrase 'trump news'. the financial times is reporting on how the public resignation of france's environment minister, nicolas hulot, is a serious blow to president emmanuel macron's image as the eu's champion on climate\n",
            "Document : /content/drive/My Drive/AIR/TelevisionNews/CNN.200912.csv\n",
            "URL  :  https://archive.org/details/CNN_20091218_030000_Anderson_Cooper_360#start/5672/end/5707\n",
            "MatchDateTime  :  12/18/2009 4:34:47\n",
            "Station  :  CNN\n",
            "Show  :  Anderson Cooper 360\n",
            "IAShowID  :  CNN_20091218_030000_Anderson_Cooper_360\n",
            "IAPreviewThumb  :  https://archive.org/download/CNN_20091218_030000_Anderson_Cooper_360/CNN_20091218_030000_Anderson_Cooper_360.thumbs/CNN_20091218_030000_Anderson_Cooper_360_005665.jpg\n",
            "Snippet  :  according to the google moderator, questions the validity of a global carbon tax.\n",
            "Document : /content/drive/My Drive/AIR/TelevisionNews/CNN.200912.csv\n",
            "URL  :  https://archive.org/details/CNN_20091208_210000_The_Situation_Room_With_Wolf_Blitzer#start/6481/end/6516\n",
            "MatchDateTime  :  12/8/2009 22:48:16\n",
            "Station  :  CNN\n",
            "Show  :  The Situation Room With Wolf Blitzer\n",
            "IAShowID  :  CNN_20091208_210000_The_Situation_Room_With_Wolf_Blitzer\n",
            "IAPreviewThumb  :  https://archive.org/download/CNN_20091208_210000_The_Situation_Room_With_Wolf_Blitzer/CNN_20091208_210000_The_Situation_Room_With_Wolf_Blitzer.thumbs/CNN_20091208_210000_The_Situation_Room_With_Wolf_Blitzer_006472.jpg\n",
            "Snippet  :  surrounding global warming, trick or truth, 8:00 p.m. eastern later tonight. it's a massive online scam using the name of the world's most popular search engine. now google is fighting back. we have the details of the lawsuit. and what should be done with the leftover $200 billion of\n",
            "Document : /content/drive/My Drive/AIR/TelevisionNews/CNN.200912.csv\n",
            "URL  :  https://archive.org/details/CNN_20091210_110000_American_Morning#start/10146/end/10181\n",
            "MatchDateTime  :  12/10/2009 13:49:21\n",
            "Station  :  CNN\n",
            "Show  :  American Morning\n",
            "IAShowID  :  CNN_20091210_110000_American_Morning\n",
            "IAPreviewThumb  :  https://archive.org/download/CNN_20091210_110000_American_Morning/CNN_20091210_110000_American_Morning.thumbs/CNN_20091210_110000_American_Morning_010132.jpg\n",
            "Snippet  :  the earth are reaching their life span, and without new funding, nasa scientists worry the satellites will not n replaced. google nasa climate change, and you can look at the eyes on\n",
            "Document : /content/drive/My Drive/AIR/TelevisionNews/CNN.200912.csv\n",
            "URL  :  https://archive.org/details/CNN_20091203_180000_CNN_Newsroom#start/5289/end/5324\n",
            "MatchDateTime  :  12/3/2009 19:28:24\n",
            "Station  :  CNN\n",
            "Show  :  CNN Newsroom\n",
            "IAShowID  :  CNN_20091203_180000_CNN_Newsroom\n",
            "IAPreviewThumb  :  https://archive.org/download/CNN_20091203_180000_CNN_Newsroom/CNN_20091203_180000_CNN_Newsroom.thumbs/CNN_20091203_180000_CNN_Newsroom_005275.jpg\n",
            "Snippet  :  that everybody has come to rely on, they actually need some energy. dan riker heads up google's climate change and energy initiatives. his team has come up with a simple thing that it hopes will\n",
            "Document : /content/drive/My Drive/AIR/TelevisionNews/CNN.200912.csv\n",
            "URL  :  https://archive.org/details/CNN_20091203_030000_Anderson_Cooper_360#start/6775/end/6810\n",
            "MatchDateTime  :  12/3/2009 4:53:10\n",
            "Station  :  CNN\n",
            "Show  :  Anderson Cooper 360\n",
            "IAShowID  :  CNN_20091203_030000_Anderson_Cooper_360\n",
            "IAPreviewThumb  :  https://archive.org/download/CNN_20091203_030000_Anderson_Cooper_360/CNN_20091203_030000_Anderson_Cooper_360.thumbs/CNN_20091203_030000_Anderson_Cooper_360_006750.jpg\n",
            "Snippet  :  reporter: dan heads up google's climate change and energy initiative. he came up with a simple thing that he hopes will save you money and save the planet along the way. it is google power meter. every time you flip on a light\n",
            "Document : /content/drive/My Drive/AIR/TelevisionNews/BBCNEWS.201909.csv\n",
            "URL  :  https://archive.org/details/BBCNEWS_20190920_130000_Afternoon_Live#start/2873/end/2908\n",
            "MatchDateTime  :  9/20/2019 13:48:08\n",
            "Station  :  BBCNEWS\n",
            "Show  :  Afternoon Live\n",
            "IAShowID  :  BBCNEWS_20190920_130000_Afternoon_Live\n",
            "IAPreviewThumb  :  https://archive.org/download/BBCNEWS_20190920_130000_Afternoon_Live/BBCNEWS_20190920_130000_Afternoon_Live.thumbs/BBCNEWS_20190920_130000_Afternoon_Live_002847.jpg\n",
            "Snippet  :  and climate change commitments - some of the world's largest firms have promised big-spending on green energy plans. amazon has pledged to be carbon neutral by 2040 and google says it will make record renewable energy purchases. those announcements coincide with today's 'climate strike' day -  _\n",
            "Document : /content/drive/My Drive/AIR/TelevisionNews/BBCNEWS.201909.csv\n",
            "URL  :  https://archive.org/details/BBCNEWS_20190920_130000_Afternoon_Live#start/6651/end/6686\n",
            "MatchDateTime  :  9/20/2019 14:51:06\n",
            "Station  :  BBCNEWS\n",
            "Show  :  Afternoon Live\n",
            "IAShowID  :  BBCNEWS_20190920_130000_Afternoon_Live\n",
            "IAPreviewThumb  :  https://archive.org/download/BBCNEWS_20190920_130000_Afternoon_Live/BBCNEWS_20190920_130000_Afternoon_Live.thumbs/BBCNEWS_20190920_130000_Afternoon_Live_006627.jpg\n",
            "Snippet  :  and climate change commitments - some of the world's largest firms have promised big-spending on green energy plans. amazon has pledged to be carbon neutral by 2040 and google says it will make record renewable energy purchases. those announcements coincide with today's 'climate strike' day -  _\n",
            "Document : /content/drive/My Drive/AIR/TelevisionNews/BBCNEWS.201912.csv\n",
            "URL  :  https://archive.org/details/BBCNEWS_20191205_010000_Newsday#start/899/end/934\n",
            "MatchDateTime  :  12/5/2019 1:15:14\n",
            "Station  :  BBCNEWS\n",
            "Show  :  Newsday\n",
            "IAShowID  :  BBCNEWS_20191205_010000_Newsday\n",
            "IAPreviewThumb  :  https://archive.org/download/BBCNEWS_20191205_010000_Newsday/BBCNEWS_20191205_010000_Newsday.thumbs/BBCNEWS_20191205_010000_Newsday_000898.jpg\n",
            "Snippet  :  animals will adapt to climate change. let's take a look at some front pages from around the world. india's business standard reports that google's founders larry page\n",
            "Document : /content/drive/My Drive/AIR/TelevisionNews/BBCNEWS.201912.csv\n",
            "URL  :  https://archive.org/details/BBCNEWS_20191205_000000_Newsday#start/913/end/948\n",
            "MatchDateTime  :  12/5/2019 0:15:28\n",
            "Station  :  BBCNEWS\n",
            "Show  :  Newsday\n",
            "IAShowID  :  BBCNEWS_20191205_000000_Newsday\n",
            "IAPreviewThumb  :  https://archive.org/download/BBCNEWS_20191205_000000_Newsday/BBCNEWS_20191205_000000_Newsday.thumbs/BBCNEWS_20191205_000000_Newsday_000898.jpg\n",
            "Snippet  :  species were getting smaller. they say the findings are important to understand how animals will adapt to climate change. let's take a look at some front pages from around the world. india's business standard reports that google's founders larry page\n",
            "Document : /content/drive/My Drive/AIR/TelevisionNews/BBCNEWS.201810.csv\n",
            "URL  :  https://archive.org/details/BBCNEWS_20181007_223000_The_Papers#start/601/end/636\n",
            "MatchDateTime  :  10/7/2018 22:40:16\n",
            "Station  :  BBCNEWS\n",
            "Show  :  The Papers\n",
            "IAShowID  :  BBCNEWS_20181007_223000_The_Papers\n",
            "IAPreviewThumb  :  https://archive.org/download/BBCNEWS_20181007_223000_The_Papers/BBCNEWS_20181007_223000_The_Papers.thumbs/BBCNEWS_20181007_223000_The_Papers_000597.jpg\n",
            "Snippet  :  consensus. we thought we did with paris but then president trump decided he would withdraw because president trump thinks that climate change is a hoax created by the chinese. google that, i'm not making it up. his own administration is saying they expect global\n",
            "Document : /content/drive/My Drive/AIR/TelevisionNews/BBCNEWS.202001.csv\n",
            "URL  :  https://archive.org/details/BBCNEWS_20200117_140000_Afternoon_Live#start/3069/end/3104\n",
            "MatchDateTime  :  1/17/2020 14:51:24\n",
            "Station  :  BBCNEWS\n",
            "Show  :  Afternoon Live\n",
            "IAShowID  :  BBCNEWS_20200117_140000_Afternoon_Live\n",
            "IAPreviewThumb  :  https://archive.org/download/BBCNEWS_20200117_140000_Afternoon_Live/BBCNEWS_20200117_140000_Afternoon_Live.thumbs/BBCNEWS_20200117_140000_Afternoon_Live_003057.jpg\n",
            "Snippet  :  emissions, orjust by not using greenhouse gases in the first place, using renewable energies instead. until now, most companies choose to offset carbon emissions. google invest in their own green projects in order to achieve neutrality. the\n",
            "Document : /content/drive/My Drive/AIR/TelevisionNews/BBCNEWS.202001.csv\n",
            "URL  :  https://archive.org/details/BBCNEWS_20200121_013000_Asia_Business_Report#start/387/end/422\n",
            "MatchDateTime  :  1/21/2020 1:36:42\n",
            "Station  :  BBCNEWS\n",
            "Show  :  Asia Business Report\n",
            "IAShowID  :  BBCNEWS_20200121_013000_Asia_Business_Report\n",
            "IAPreviewThumb  :  https://archive.org/download/BBCNEWS_20200121_013000_Asia_Business_Report/BBCNEWS_20200121_013000_Asia_Business_Report.thumbs/BBCNEWS_20200121_013000_Asia_Business_Report_000387.jpg\n",
            "Snippet  :  climate activist greta thunberg and top executives from companies including google, netflix and coca-cola. but is there the will to be able to tackle the issue of climate change? there's clearly a business impact because a report by the imf and cambridge university\n",
            "Document : /content/drive/My Drive/AIR/TelevisionNews/BBCNEWS.202001.csv\n",
            "URL  :  https://archive.org/details/BBCNEWS_20200121_053000_Business_Briefing#start/88/end/123\n",
            "MatchDateTime  :  1/21/2020 5:31:43\n",
            "Station  :  BBCNEWS\n",
            "Show  :  Business Briefing\n",
            "IAShowID  :  BBCNEWS_20200121_053000_Business_Briefing\n",
            "IAPreviewThumb  :  https://archive.org/download/BBCNEWS_20200121_053000_Business_Briefing/BBCNEWS_20200121_053000_Business_Briefing.thumbs/BBCNEWS_20200121_053000_Business_Briefing_000087.jpg\n",
            "Snippet  :  at this year's world economic forum, climate change is the hot topic of the week. attending this year will be president trump, teenage climate activist greta thunberg and top executives from firms including google, netflix, coca-cola but is there the will to tackle the issue? there's clearly a business impact,\n",
            "Document : /content/drive/My Drive/AIR/TelevisionNews/CNN.201209.csv\n",
            "URL  :  https://archive.org/details/CNNW_20120930_120000_CNN_Saturday_Morning#start/2640/end/2675\n",
            "MatchDateTime  :  9/30/2012 12:44:15\n",
            "Station  :  CNN\n",
            "Show  :  CNN Saturday Morning\n",
            "IAShowID  :  CNNW_20120930_120000_CNN_Saturday_Morning\n",
            "IAPreviewThumb  :  https://archive.org/download/CNNW_20120930_120000_CNN_Saturday_Morning/CNNW_20120930_120000_CNN_Saturday_Morning.thumbs/CNNW_20120930_120000_CNN_Saturday_Morning_002638.jpg\n",
            "Snippet  :  getting these panoramic views in high definition. so why is google doing this? google is doing this in partnership with the scientists, and the scientists specifically are saying we want to create awareness about the reef, but what is the impact of climate change is having on the 2300\n",
            "Document : /content/drive/My Drive/AIR/TelevisionNews/CNN.201901.csv\n",
            "URL  :  https://archive.org/details/CNNW_20190130_060000_Cuomo_Prime_Time#start/2927/end/2962\n",
            "MatchDateTime  :  1/30/2019 6:49:02\n",
            "Station  :  CNN\n",
            "Show  :  Cuomo Prime Time\n",
            "IAShowID  :  CNNW_20190130_060000_Cuomo_Prime_Time\n",
            "IAPreviewThumb  :  https://archive.org/download/CNNW_20190130_060000_Cuomo_Prime_Time/CNNW_20190130_060000_Cuomo_Prime_Time.thumbs/CNNW_20190130_060000_Cuomo_Prime_Time_002908.jpg\n",
            "Snippet  :  that's extra credit for this president. so we'll assume we know what he's talking about. facts first, climate change hasn't gone anywhere. weather and climate. okay. you have to separate those a little bit in the analysis. you can google it.\n",
            "Document : /content/drive/My Drive/AIR/TelevisionNews/FOXNEWS.200912.csv\n",
            "URL  :  https://archive.org/details/FOXNEWS_20091210_090000_Special_Report_With_Bret_Baier#start/3563/end/3598\n",
            "MatchDateTime  :  12/10/2009 9:59:38\n",
            "Station  :  FOXNEWS\n",
            "Show  :  Special Report With Bret Baier\n",
            "IAShowID  :  FOXNEWS_20091210_090000_Special_Report_With_Bret_Baier\n",
            "IAPreviewThumb  :  https://archive.org/download/FOXNEWS_20091210_090000_Special_Report_With_Bret_Baier/FOXNEWS_20091210_090000_Special_Report_With_Bret_Baier.thumbs/FOXNEWS_20091210_090000_Special_Report_With_Bret_Baier_003562.jpg\n",
            "Snippet  :  governor arnold schwarzenegger announced wednesday google is developing a tool to map out disturbing scenarios how california can be affected by climate change and called cal-adapt and allows californians to see how global warming impacts their\n",
            "Document : /content/drive/My Drive/AIR/TelevisionNews/FOXNEWS.200912.csv\n",
            "URL  :  https://archive.org/details/FOXNEWS_20091209_230000_Special_Report_With_Bret_Baier#start/3565/end/3600\n",
            "MatchDateTime  :  12/9/2009 23:59:40\n",
            "Station  :  FOXNEWS\n",
            "Show  :  Special Report With Bret Baier\n",
            "IAShowID  :  FOXNEWS_20091209_230000_Special_Report_With_Bret_Baier\n",
            "IAPreviewThumb  :  https://archive.org/download/FOXNEWS_20091209_230000_Special_Report_With_Bret_Baier/FOXNEWS_20091209_230000_Special_Report_With_Bret_Baier.thumbs/FOXNEWS_20091209_230000_Special_Report_With_Bret_Baier_003562.jpg\n",
            "Snippet  :  governor arnold schwarzenegger announced wednesday google is developing a tool to map out disturbing scenarios how california can be affected by climate change and called cal-adapt and allows californians to see how global warming impacts their\n",
            "Document : /content/drive/My Drive/AIR/TelevisionNews/FOXNEWS.201006.csv\n",
            "URL  :  https://archive.org/details/FOXNEWS_20100630_040000_Hannity#start/1715/end/1750\n",
            "MatchDateTime  :  6/30/2010 4:28:50\n",
            "Station  :  FOXNEWS\n",
            "Show  :  Hannity\n",
            "IAShowID  :  FOXNEWS_20100630_040000_Hannity\n",
            "IAPreviewThumb  :  https://archive.org/download/FOXNEWS_20100630_040000_Hannity/FOXNEWS_20100630_040000_Hannity.thumbs/FOXNEWS_20100630_040000_Hannity_001705.jpg\n",
            "Snippet  :  going to cover the story if you google al gore and sex poodle almost nothing comes up. you are the only one on the story. he's expected to get cover while he around the country and starts screaming about climate change.\n",
            "Document : /content/drive/My Drive/AIR/TelevisionNews/FOXNEWS.201909.csv\n",
            "URL  :  https://archive.org/details/FOXNEWSW_20190908_210000_The_Greg_Gutfeld_Show#start/2494/end/2529\n",
            "MatchDateTime  :  9/8/2019 21:41:49\n",
            "Station  :  FOXNEWS\n",
            "Show  :  The Greg Gutfeld Show\n",
            "IAShowID  :  FOXNEWSW_20190908_210000_The_Greg_Gutfeld_Show\n",
            "IAPreviewThumb  :  https://archive.org/download/FOXNEWSW_20190908_210000_The_Greg_Gutfeld_Show/FOXNEWSW_20190908_210000_The_Greg_Gutfeld_Show.thumbs/FOXNEWSW_20190908_210000_The_Greg_Gutfeld_Show_002488.jpg\n",
            "Snippet  :  change being involved and people giving up google searches i would not vote for the climate change. try david petraeus i would say hoodies and sunblock. we can always go to mars.\n",
            "Document : /content/drive/My Drive/AIR/TelevisionNews/FOXNEWS.201909.csv\n",
            "URL  :  https://archive.org/details/FOXNEWSW_20190908_050000_The_Greg_Gutfeld_Show#start/2497/end/2532\n",
            "MatchDateTime  :  9/8/2019 5:41:52\n",
            "Station  :  FOXNEWS\n",
            "Show  :  The Greg Gutfeld Show\n",
            "IAShowID  :  FOXNEWSW_20190908_050000_The_Greg_Gutfeld_Show\n",
            "IAPreviewThumb  :  https://archive.org/download/FOXNEWSW_20190908_050000_The_Greg_Gutfeld_Show/FOXNEWSW_20190908_050000_The_Greg_Gutfeld_Show.thumbs/FOXNEWSW_20190908_050000_The_Greg_Gutfeld_Show_002489.jpg\n",
            "Snippet  :  change being involved and people giving up google searches i would not vote for the climate change. try david petraeus i would say hoodies and sunblock. we can always go to mars.\n",
            "Document : /content/drive/My Drive/AIR/TelevisionNews/FOXNEWS.201909.csv\n",
            "URL  :  https://archive.org/details/FOXNEWSW_20190908_080000_The_Greg_Gutfeld_Show#start/2494/end/2529\n",
            "MatchDateTime  :  9/8/2019 8:41:49\n",
            "Station  :  FOXNEWS\n",
            "Show  :  The Greg Gutfeld Show\n",
            "IAShowID  :  FOXNEWSW_20190908_080000_The_Greg_Gutfeld_Show\n",
            "IAPreviewThumb  :  https://archive.org/download/FOXNEWSW_20190908_080000_The_Greg_Gutfeld_Show/FOXNEWSW_20190908_080000_The_Greg_Gutfeld_Show.thumbs/FOXNEWSW_20190908_080000_The_Greg_Gutfeld_Show_002488.jpg\n",
            "Snippet  :  change being involved and people giving up google searches i would not vote for the climate change. try david petraeus i would say hoodies and sunblock. we can always go to mars.\n",
            "Document : /content/drive/My Drive/AIR/TelevisionNews/FOXNEWS.201909.csv\n",
            "URL  :  https://archive.org/details/FOXNEWSW_20190908_020000_The_Greg_Gutfeld_Show#start/2494/end/2529\n",
            "MatchDateTime  :  9/8/2019 2:41:49\n",
            "Station  :  FOXNEWS\n",
            "Show  :  The Greg Gutfeld Show\n",
            "IAShowID  :  FOXNEWSW_20190908_020000_The_Greg_Gutfeld_Show\n",
            "IAPreviewThumb  :  https://archive.org/download/FOXNEWSW_20190908_020000_The_Greg_Gutfeld_Show/FOXNEWSW_20190908_020000_The_Greg_Gutfeld_Show.thumbs/FOXNEWSW_20190908_020000_The_Greg_Gutfeld_Show_002488.jpg\n",
            "Snippet  :  change being involved and people giving up google searches i would not vote for the climate change. try david petraeus i would say hoodies and sunblock. we can always go to mars.\n",
            "Document : /content/drive/My Drive/AIR/TelevisionNews/FOXNEWS.201909.csv\n",
            "URL  :  https://archive.org/details/FOXNEWSW_20190908_210000_The_Greg_Gutfeld_Show#start/2704/end/2739\n",
            "MatchDateTime  :  9/8/2019 21:45:19\n",
            "Station  :  FOXNEWS\n",
            "Show  :  The Greg Gutfeld Show\n",
            "IAShowID  :  FOXNEWSW_20190908_210000_The_Greg_Gutfeld_Show\n",
            "IAPreviewThumb  :  https://archive.org/download/FOXNEWSW_20190908_210000_The_Greg_Gutfeld_Show/FOXNEWSW_20190908_210000_The_Greg_Gutfeld_Show.thumbs/FOXNEWSW_20190908_210000_The_Greg_Gutfeld_Show_002698.jpg\n",
            "Snippet  :  i didn't go there, you did. kat: you always have to be clear. you could really affect climate change if you shut down google. you think about -- if they did the research, each search is\n",
            "Document : /content/drive/My Drive/AIR/TelevisionNews/FOXNEWS.201909.csv\n",
            "URL  :  https://archive.org/details/FOXNEWSW_20190908_080000_The_Greg_Gutfeld_Show#start/2705/end/2740\n",
            "MatchDateTime  :  9/8/2019 8:45:20\n",
            "Station  :  FOXNEWS\n",
            "Show  :  The Greg Gutfeld Show\n",
            "IAShowID  :  FOXNEWSW_20190908_080000_The_Greg_Gutfeld_Show\n",
            "IAPreviewThumb  :  https://archive.org/download/FOXNEWSW_20190908_080000_The_Greg_Gutfeld_Show/FOXNEWSW_20190908_080000_The_Greg_Gutfeld_Show.thumbs/FOXNEWSW_20190908_080000_The_Greg_Gutfeld_Show_002698.jpg\n",
            "Snippet  :  i didn't go there, you did. kat: you always have to be clear. you could really affect climate change if you shut down google. you think about -- if they did the research, each search is\n",
            "Document : /content/drive/My Drive/AIR/TelevisionNews/FOXNEWS.201909.csv\n",
            "URL  :  https://archive.org/details/FOXNEWSW_20190908_050000_The_Greg_Gutfeld_Show#start/2702/end/2737\n",
            "MatchDateTime  :  9/8/2019 5:45:17\n",
            "Station  :  FOXNEWS\n",
            "Show  :  The Greg Gutfeld Show\n",
            "IAShowID  :  FOXNEWSW_20190908_050000_The_Greg_Gutfeld_Show\n",
            "IAPreviewThumb  :  https://archive.org/download/FOXNEWSW_20190908_050000_The_Greg_Gutfeld_Show/FOXNEWSW_20190908_050000_The_Greg_Gutfeld_Show.thumbs/FOXNEWSW_20190908_050000_The_Greg_Gutfeld_Show_002697.jpg\n",
            "Snippet  :  doing that. greg: unless there are three scandinavian models in the boat. i didn't go there, you did. kat: you always have to be clear. you could really affect climate change if you shut down google.\n",
            "Document : /content/drive/My Drive/AIR/TelevisionNews/FOXNEWS.201909.csv\n",
            "URL  :  https://archive.org/details/FOXNEWSW_20190908_020000_The_Greg_Gutfeld_Show#start/2699/end/2734\n",
            "MatchDateTime  :  9/8/2019 2:45:14\n",
            "Station  :  FOXNEWS\n",
            "Show  :  The Greg Gutfeld Show\n",
            "IAShowID  :  FOXNEWSW_20190908_020000_The_Greg_Gutfeld_Show\n",
            "IAPreviewThumb  :  https://archive.org/download/FOXNEWSW_20190908_020000_The_Greg_Gutfeld_Show/FOXNEWSW_20190908_020000_The_Greg_Gutfeld_Show.thumbs/FOXNEWSW_20190908_020000_The_Greg_Gutfeld_Show_002698.jpg\n",
            "Snippet  :  doing that. greg: unless there are three scandinavian models in the boat. i didn't go there, you did. kat: you always have to be clear. you could really affect climate change if you shut down google.\n",
            "Document : /content/drive/My Drive/AIR/TelevisionNews/FOXNEWS.201912.csv\n",
            "URL  :  https://archive.org/details/FOXNEWSW_20191210_220000_The_Five#start/3398/end/3433\n",
            "MatchDateTime  :  12/10/2019 22:56:53\n",
            "Station  :  FOXNEWS\n",
            "Show  :  The Five\n",
            "IAShowID  :  FOXNEWSW_20191210_220000_The_Five\n",
            "IAPreviewThumb  :  https://archive.org/download/FOXNEWSW_20191210_220000_The_Five/FOXNEWSW_20191210_220000_The_Five.thumbs/FOXNEWSW_20191210_220000_The_Five_003389.jpg\n",
            "Snippet  :  had the wrong thing but steve harvey is hilarious. you have to google when he asked the winner the question about climate change and watch him. he's hilarious. pretty funny. the winner was south africa's\n",
            "Document : /content/drive/My Drive/AIR/TelevisionNews/FOXNEWS.201110.csv\n",
            "URL  :  https://archive.org/details/FOXNEWSW_20111008_180000_The_Journal_Editorial_Report#start/872/end/907\n",
            "MatchDateTime  :  10/8/2011 18:14:47\n",
            "Station  :  FOXNEWS\n",
            "Show  :  The Journal Editorial Report\n",
            "IAShowID  :  FOXNEWSW_20111008_180000_The_Journal_Editorial_Report\n",
            "IAPreviewThumb  :  https://archive.org/download/FOXNEWSW_20111008_180000_The_Journal_Editorial_Report/FOXNEWSW_20111008_180000_The_Journal_Editorial_Report.thumbs/FOXNEWSW_20111008_180000_The_Journal_Editorial_Report_000867.jpg\n",
            "Snippet  :  philosophically, if you look at old film clips on mitt romney from the '90s and 2000's, conservative, pro-abortion, global warming stuff, gun control he's moved off the positions now and go back and google herman cain\n",
            "Document : /content/drive/My Drive/AIR/TelevisionNews/FOXNEWS.201110.csv\n",
            "URL  :  https://archive.org/details/FOXNEWSW_20111009_030000_The_Journal_Editorial_Report#start/872/end/907\n",
            "MatchDateTime  :  10/9/2011 3:14:47\n",
            "Station  :  FOXNEWS\n",
            "Show  :  The Journal Editorial Report\n",
            "IAShowID  :  FOXNEWSW_20111009_030000_The_Journal_Editorial_Report\n",
            "IAPreviewThumb  :  https://archive.org/download/FOXNEWSW_20111009_030000_The_Journal_Editorial_Report/FOXNEWSW_20111009_030000_The_Journal_Editorial_Report.thumbs/FOXNEWSW_20111009_030000_The_Journal_Editorial_Report_000867.jpg\n",
            "Snippet  :  philosophically, if you look at old film clips on mitt romney from the '90s and 2000's, conservative, pro-abortion, global warming stuff, gun control he's moved off the positions now and go back and google herman cain\n",
            "Document : /content/drive/My Drive/AIR/TelevisionNews/FOXNEWS.201403.csv\n",
            "URL  :  https://archive.org/details/FOXNEWSW_20140328_100000_FOX_and_Friends#start/2815/end/2850\n",
            "MatchDateTime  :  3/28/2014 10:47:10\n",
            "Station  :  FOXNEWS\n",
            "Show  :  FOX and Friends\n",
            "IAShowID  :  FOXNEWSW_20140328_100000_FOX_and_Friends\n",
            "IAPreviewThumb  :  https://archive.org/download/FOXNEWSW_20140328_100000_FOX_and_Friends/FOXNEWSW_20140328_100000_FOX_and_Friends.thumbs/FOXNEWSW_20140328_100000_FOX_and_Friends_002805.jpg\n",
            "Snippet  :  120%, how much the u.s. requests for government information from google have risen since 2009. more than 10000 were made over a five month span last year. 700 thousand dollars, that's how much the government gave a theater company to make this musical about global\n",
            "Document : /content/drive/My Drive/AIR/TelevisionNews/CNN.201811.csv\n",
            "URL  :  https://archive.org/details/CNNW_20181124_160000_CNN_Newsroom_With_Fredricka_Whitfield#start/1438/end/1473\n",
            "MatchDateTime  :  11/24/2018 16:24:13\n",
            "Station  :  CNN\n",
            "Show  :  CNN Newsroom With Fredricka Whitfield\n",
            "IAShowID  :  CNNW_20181124_160000_CNN_Newsroom_With_Fredricka_Whitfield\n",
            "IAPreviewThumb  :  https://archive.org/download/CNNW_20181124_160000_CNN_Newsroom_With_Fredricka_Whitfield/CNNW_20181124_160000_CNN_Newsroom_With_Fredricka_Whitfield.thumbs/CNNW_20181124_160000_CNN_Newsroom_With_Fredricka_Whitfield_001438.jpg\n",
            "Snippet  :  english if people want to google it. scientific findings, beyond theories, scientific findings. which are there. there is an executive summary at the top you can read that is not technical and for an administration that wants to scrub the words climate change\n",
            "Document : /content/drive/My Drive/AIR/TelevisionNews/FOXNEWS.201106.csv\n",
            "URL  :  https://archive.org/details/FOXNEWS_20110603_100000_FOX_and_Friends#start/6327/end/6362\n",
            "MatchDateTime  :  6/3/2011 11:45:42\n",
            "Station  :  FOXNEWS\n",
            "Show  :  FOX and Friends\n",
            "IAShowID  :  FOXNEWS_20110603_100000_FOX_and_Friends\n",
            "IAPreviewThumb  :  https://archive.org/download/FOXNEWS_20110603_100000_FOX_and_Friends/FOXNEWS_20110603_100000_FOX_and_Friends.thumbs/FOXNEWS_20110603_100000_FOX_and_Friends_006289.jpg\n",
            "Snippet  :  challenged his company's annual shareholders meeting for not disclosing google investments in the green energy projects of a fellow board member. a global warming law proponent and al gore's business partner. how could that happen? fox news contributor daneen\n",
            "Document : /content/drive/My Drive/AIR/TelevisionNews/FOXNEWS.201106.csv\n",
            "URL  :  https://archive.org/details/FOXNEWS_20110603_100000_FOX_and_Friends#start/6076/end/6111\n",
            "MatchDateTime  :  6/3/2011 11:41:31\n",
            "Station  :  FOXNEWS\n",
            "Show  :  FOX and Friends\n",
            "IAShowID  :  FOXNEWS_20110603_100000_FOX_and_Friends\n",
            "IAPreviewThumb  :  https://archive.org/download/FOXNEWS_20110603_100000_FOX_and_Friends/FOXNEWS_20110603_100000_FOX_and_Friends.thumbs/FOXNEWS_20110603_100000_FOX_and_Friends_006074.jpg\n",
            "Snippet  :  not having a date at in and out burger, ok? never having a date at in and out burger. it gets too messy. they take debit cards. they'll see me. next on the rundown, google's ties to global warming and al gore making for a very\n",
            "Document : /content/drive/My Drive/AIR/TelevisionNews/FOXNEWS.201106.csv\n",
            "URL  :  https://archive.org/details/FOXNEWS_20110603_100000_FOX_and_Friends#start/6422/end/6457\n",
            "MatchDateTime  :  6/3/2011 11:47:17\n",
            "Station  :  FOXNEWS\n",
            "Show  :  FOX and Friends\n",
            "IAShowID  :  FOXNEWS_20110603_100000_FOX_and_Friends\n",
            "IAPreviewThumb  :  https://archive.org/download/FOXNEWS_20110603_100000_FOX_and_Friends/FOXNEWS_20110603_100000_FOX_and_Friends.thumbs/FOXNEWS_20110603_100000_FOX_and_Friends_006397.jpg\n",
            "Snippet  :  and john dorr is also on the president -- one of the president's panels as well, right? in the president's ear, he is someone who supports climate change legislation. well, now, you say it's a conflict of interest. here's what google sent in an e-mail to us. in the event of annual conflict\n",
            "Document : /content/drive/My Drive/AIR/TelevisionNews/FOXNEWS.201801.csv\n",
            "URL  :  https://archive.org/details/FOXNEWSW_20180110_170000_Outnumbered#start/3206/end/3241\n",
            "MatchDateTime  :  1/10/2018 17:53:41\n",
            "Station  :  FOXNEWS\n",
            "Show  :  Outnumbered\n",
            "IAShowID  :  FOXNEWSW_20180110_170000_Outnumbered\n",
            "IAPreviewThumb  :  https://archive.org/download/FOXNEWSW_20180110_170000_Outnumbered/FOXNEWSW_20180110_170000_Outnumbered.thumbs/FOXNEWSW_20180110_170000_Outnumbered_003207.jpg\n",
            "Snippet  :  that right man made global warming, how much of a factor it is actually a subject of some disagreement. there is no substantive disagreement that there is climate change. how many man's role goes into climate change is a matter of some dispute. when google gets into saying\n",
            "Document : /content/drive/My Drive/AIR/TelevisionNews/FOXNEWS.201302.csv\n",
            "URL  :  https://archive.org/details/FOXNEWSW_20130202_070000_The_Five#start/1253/end/1288\n",
            "MatchDateTime  :  2/2/2013 7:21:08\n",
            "Station  :  FOXNEWS\n",
            "Show  :  The Five\n",
            "IAShowID  :  FOXNEWSW_20130202_070000_The_Five\n",
            "IAPreviewThumb  :  https://archive.org/download/FOXNEWSW_20130202_070000_The_Five/FOXNEWSW_20130202_070000_The_Five.thumbs/FOXNEWSW_20130202_070000_The_Five_001245.jpg\n",
            "Snippet  :  i'm not sure. it ought to know but i don't know. wait, i can google it. yeah. are you al gore or are you in business with this country that is enabling the ultimate climate change -- i think i understand what\n",
            "Document : /content/drive/My Drive/AIR/TelevisionNews/FOXNEWS.201302.csv\n",
            "URL  :  https://archive.org/details/FOXNEWSW_20130201_220000_The_Five#start/1254/end/1289\n",
            "MatchDateTime  :  2/1/2013 22:21:09\n",
            "Station  :  FOXNEWS\n",
            "Show  :  The Five\n",
            "IAShowID  :  FOXNEWSW_20130201_220000_The_Five\n",
            "IAPreviewThumb  :  https://archive.org/download/FOXNEWSW_20130201_220000_The_Five/FOXNEWSW_20130201_220000_The_Five.thumbs/FOXNEWSW_20130201_220000_The_Five_001245.jpg\n",
            "Snippet  :  i'm not sure. it ought to know but i don't know. wait, i can google it. yeah. are you al gore or are you in business with this country that is enabling the ultimate climate change -- i think i understand what you are getting at.\n",
            "Document : /content/drive/My Drive/AIR/TelevisionNews/FOXNEWS.201904.csv\n",
            "URL  :  https://archive.org/details/FOXNEWSW_20190402_000000_Tucker_Carlson_Tonight#start/647/end/682\n",
            "MatchDateTime  :  4/2/2019 0:11:02\n",
            "Station  :  FOXNEWS\n",
            "Show  :  Tucker Carlson Tonight\n",
            "IAShowID  :  FOXNEWSW_20190402_000000_Tucker_Carlson_Tonight\n",
            "IAPreviewThumb  :  https://archive.org/download/FOXNEWSW_20190402_000000_Tucker_Carlson_Tonight/FOXNEWSW_20190402_000000_Tucker_Carlson_Tonight.thumbs/FOXNEWSW_20190402_000000_Tucker_Carlson_Tonight_000630.jpg\n",
            "Snippet  :  in cities like new york. just in 10 years in cities like new york. tucker: is there a website 8.9% of scientists. www.google.com scientists who understand climate change.\n",
            "Document : /content/drive/My Drive/AIR/TelevisionNews/FOXNEWS.201904.csv\n",
            "URL  :  https://archive.org/details/FOXNEWSW_20190402_040000_Tucker_Carlson_Tonight#start/661/end/696\n",
            "MatchDateTime  :  4/2/2019 4:11:16\n",
            "Station  :  FOXNEWS\n",
            "Show  :  Tucker Carlson Tonight\n",
            "IAShowID  :  FOXNEWSW_20190402_040000_Tucker_Carlson_Tonight\n",
            "IAPreviewThumb  :  https://archive.org/download/FOXNEWSW_20190402_040000_Tucker_Carlson_Tonight/FOXNEWSW_20190402_040000_Tucker_Carlson_Tonight.thumbs/FOXNEWSW_20190402_040000_Tucker_Carlson_Tonight_000658.jpg\n",
            "Snippet  :  google.com. scientists who understand climate change. tucker: is there a membership committee that allows us? the association of scientist scientists. tucker: okay, great. will checkec that. i want to thank you and the association of scientists.\n",
            "Document : /content/drive/My Drive/AIR/TelevisionNews/FOXNEWS.201301.csv\n",
            "URL  :  https://archive.org/details/FOXNEWSW_20130111_110000_FOX_and_Friends#start/2970/end/3005\n",
            "MatchDateTime  :  1/11/2013 11:49:45\n",
            "Station  :  FOXNEWS\n",
            "Show  :  FOX and Friends\n",
            "IAShowID  :  FOXNEWSW_20130111_110000_FOX_and_Friends\n",
            "IAPreviewThumb  :  https://archive.org/download/FOXNEWSW_20130111_110000_FOX_and_Friends/FOXNEWSW_20130111_110000_FOX_and_Friends.thumbs/FOXNEWSW_20130111_110000_FOX_and_Friends_002955.jpg\n",
            "Snippet  :  movie is the guy who played mark twain and said google fracking. there is far more evidence in fracking than global warming.\n",
            "Document : /content/drive/My Drive/AIR/TelevisionNews/FOXNEWS.201301.csv\n",
            "URL  :  https://archive.org/details/FOXNEWSW_20130118_080000_Red_Eye#start/420/end/455\n",
            "MatchDateTime  :  1/18/2013 8:07:15\n",
            "Station  :  FOXNEWS\n",
            "Show  :  Red Eye\n",
            "IAShowID  :  FOXNEWSW_20130118_080000_Red_Eye\n",
            "IAPreviewThumb  :  https://archive.org/download/FOXNEWSW_20130118_080000_Red_Eye/FOXNEWSW_20130118_080000_Red_Eye.thumbs/FOXNEWSW_20130118_080000_Red_Eye_000405.jpg\n",
            "Snippet  :  this is not the person. no one in south bend could do it. nobody at the washington post or the new york times could use google picture. it is so basic. they didn't bother. you know what this is this it is like global warming, dana. it is like global warming.\n",
            "Document : /content/drive/My Drive/AIR/TelevisionNews/FOXNEWS.201301.csv\n",
            "URL  :  https://archive.org/details/FOXNEWSW_20130120_070000_Red_Eye#start/419/end/454\n",
            "MatchDateTime  :  1/20/2013 7:07:14\n",
            "Station  :  FOXNEWS\n",
            "Show  :  Red Eye\n",
            "IAShowID  :  FOXNEWSW_20130120_070000_Red_Eye\n",
            "IAPreviewThumb  :  https://archive.org/download/FOXNEWSW_20130120_070000_Red_Eye/FOXNEWSW_20130120_070000_Red_Eye.thumbs/FOXNEWSW_20130120_070000_Red_Eye_000405.jpg\n",
            "Snippet  :  this is not the person. no one in south bend could do it. nobody at the washington post or the new york times could use google picture. it is so basic. they didn't bother. you know what this is this it is like global warming,\n",
            "Document : /content/drive/My Drive/AIR/TelevisionNews/FOXNEWS.201808.csv\n",
            "URL  :  https://archive.org/details/FOXNEWSW_20180821_000000_Tucker_Carlson_Tonight#start/1788/end/1823\n",
            "MatchDateTime  :  8/21/2018 0:30:03\n",
            "Station  :  FOXNEWS\n",
            "Show  :  Tucker Carlson Tonight\n",
            "IAShowID  :  FOXNEWSW_20180821_000000_Tucker_Carlson_Tonight\n",
            "IAPreviewThumb  :  https://archive.org/download/FOXNEWSW_20180821_000000_Tucker_Carlson_Tonight/FOXNEWSW_20180821_000000_Tucker_Carlson_Tonight.thumbs/FOXNEWSW_20180821_000000_Tucker_Carlson_Tonight_001767.jpg\n",
            "Snippet  :  in this case somehow all the other controversial stuff hasn't required a disclaimer but global warming stories now do. tucker: so what have you said that youtube, google\n",
            "Document : /content/drive/My Drive/AIR/TelevisionNews/CNN.201911.csv\n",
            "URL  :  https://archive.org/details/CNNW_20191127_090000_Early_Start_with_Christine_Romans_and_Dave_Briggs#start/2424/end/2459\n",
            "MatchDateTime  :  11/27/2019 9:40:39\n",
            "Station  :  CNN\n",
            "Show  :  Early Start with Christine Romans and Dave Briggs\n",
            "IAShowID  :  CNNW_20191127_090000_Early_Start_with_Christine_Romans_and_Dave_Briggs\n",
            "IAPreviewThumb  :  https://archive.org/download/CNNW_20191127_090000_Early_Start_with_Christine_Romans_and_Dave_Briggs/CNNW_20191127_090000_Early_Start_with_Christine_Romans_and_Dave_Briggs.thumbs/CNNW_20191127_090000_Early_Start_with_Christine_Romans_and_Dave_Briggs_002399.jpg\n",
            "Snippet  :  congratulationses and issues on climate change. on monday google fired four employees. some employees are accusing google of suppressing its critics. someone tweeted google fired four of my co-works after asking\n",
            "Document : /content/drive/My Drive/AIR/TelevisionNews/MSNBC.201308.csv\n",
            "URL  :  https://archive.org/details/MSNBCW_20130804_160000_Weekends_With_Alex_Witt#start/49/end/84\n",
            "MatchDateTime  :  8/4/2013 16:01:04\n",
            "Station  :  MSNBC\n",
            "Show  :  Weekends With Alex Witt\n",
            "IAShowID  :  MSNBCW_20130804_160000_Weekends_With_Alex_Witt\n",
            "IAPreviewThumb  :  https://archive.org/download/MSNBCW_20130804_160000_Weekends_With_Alex_Witt/MSNBCW_20130804_160000_Weekends_With_Alex_Witt.thumbs/MSNBCW_20130804_160000_Weekends_With_Alex_Witt_000046.jpg\n",
            "Snippet  :  vendor, the young and the old in a seemingly intentional attack on the west coast. we will hear from a witness. climate change. a new report suggests it's causing something around the world you might never have expected. google glass experiment. what happens when you spend days\n",
            "Document : /content/drive/My Drive/AIR/TelevisionNews/MSNBC.201308.csv\n",
            "URL  :  https://archive.org/details/MSNBCW_20130806_070000_All_In_With_Chris_Hayes#start/2555/end/2590\n",
            "MatchDateTime  :  8/6/2013 7:42:50\n",
            "Station  :  MSNBC\n",
            "Show  :  All In With Chris Hayes\n",
            "IAShowID  :  MSNBCW_20130806_070000_All_In_With_Chris_Hayes\n",
            "IAPreviewThumb  :  https://archive.org/download/MSNBCW_20130806_070000_All_In_With_Chris_Hayes/MSNBCW_20130806_070000_All_In_With_Chris_Hayes.thumbs/MSNBCW_20130806_070000_All_In_With_Chris_Hayes_002537.jpg\n",
            "Snippet  :  post hopes that lab made meat can help fight climate change and feed the planet. although he admits it could take up to two decades to get lab meat into supermarkets. the project was bankrolled by the google co-founder who shares similar concerns about sustainability and animal welfare.\n",
            "Document : /content/drive/My Drive/AIR/TelevisionNews/MSNBC.201308.csv\n",
            "URL  :  https://archive.org/details/MSNBCW_20130806_000000_All_In_With_Chris_Hayes#start/2552/end/2587\n",
            "MatchDateTime  :  8/6/2013 0:42:47\n",
            "Station  :  MSNBC\n",
            "Show  :  All In With Chris Hayes\n",
            "IAShowID  :  MSNBCW_20130806_000000_All_In_With_Chris_Hayes\n",
            "IAPreviewThumb  :  https://archive.org/download/MSNBCW_20130806_000000_All_In_With_Chris_Hayes/MSNBCW_20130806_000000_All_In_With_Chris_Hayes.thumbs/MSNBCW_20130806_000000_All_In_With_Chris_Hayes_002535.jpg\n",
            "Snippet  :  post hopes that lab made meat can help fight climate change and feed the planet. although he admits it could take up to two decades to get lab meat into supermarkets. the project was bankrolled by the google co-founder who shares similar concerns about sustainability and animal\n",
            "Document : /content/drive/My Drive/AIR/TelevisionNews/MSNBC.201308.csv\n",
            "URL  :  https://archive.org/details/MSNBCW_20130806_030000_All_In_With_Chris_Hayes#start/2565/end/2600\n",
            "MatchDateTime  :  8/6/2013 3:43:00\n",
            "Station  :  MSNBC\n",
            "Show  :  All In With Chris Hayes\n",
            "IAShowID  :  MSNBCW_20130806_030000_All_In_With_Chris_Hayes\n",
            "IAPreviewThumb  :  https://archive.org/download/MSNBCW_20130806_030000_All_In_With_Chris_Hayes/MSNBCW_20130806_030000_All_In_With_Chris_Hayes.thumbs/MSNBCW_20130806_030000_All_In_With_Chris_Hayes_002566.jpg\n",
            "Snippet  :  post hopes that lab made meat can help fight climate change and feed the planet. although he admits it could take up to two decades to get lab meat into supermarkets. the project was bankrolled by the google co-founder who shares similar concerns about sustainability and animal welfare.\n",
            "Document : /content/drive/My Drive/AIR/TelevisionNews/FOXNEWS.201908.csv\n",
            "URL  :  https://archive.org/details/FOXNEWSW_20190802_100000_FOX__Friends#start/4959/end/4994\n",
            "MatchDateTime  :  8/2/2019 11:22:54\n",
            "Station  :  FOXNEWS\n",
            "Show  :  FOX  Friends\n",
            "IAShowID  :  FOXNEWSW_20190802_100000_FOX__Friends\n",
            "IAPreviewThumb  :  https://archive.org/download/FOXNEWSW_20190802_100000_FOX__Friends/FOXNEWSW_20190802_100000_FOX__Friends.thumbs/FOXNEWSW_20190802_100000_FOX__Friends_004949.jpg\n",
            "Snippet  :  steve. steve: google has invited all these people, they are spending something like $20 million to do it. they are probably since they are talking about global warming. they probably are not releasing. this the impact of the google camp attendees it has been estimated that with 114\n",
            "Document : /content/drive/My Drive/AIR/TelevisionNews/FOXNEWS.201908.csv\n",
            "URL  :  https://archive.org/details/FOXNEWSW_20190802_100000_FOX__Friends#start/4789/end/4824\n",
            "MatchDateTime  :  8/2/2019 11:20:04\n",
            "Station  :  FOXNEWS\n",
            "Show  :  FOX  Friends\n",
            "IAShowID  :  FOXNEWSW_20190802_100000_FOX__Friends\n",
            "IAPreviewThumb  :  https://archive.org/download/FOXNEWSW_20190802_100000_FOX__Friends/FOXNEWSW_20190802_100000_FOX__Friends.thumbs/FOXNEWSW_20190802_100000_FOX__Friends_004768.jpg\n",
            "Snippet  :  famous attending google's climate change conference facing backlash after flocking to the event in italy using over 100 private jets. suggested helicopters and mega yachts. joining us now the executive editor of climate depot.com\n",
            "Document : /content/drive/My Drive/AIR/TelevisionNews/MSNBC.201403.csv\n",
            "URL  :  https://archive.org/details/MSNBCW_20140319_130000_The_Daily_Rundown#start/2952/end/2987\n",
            "MatchDateTime  :  3/19/2014 13:49:27\n",
            "Station  :  MSNBC\n",
            "Show  :  The Daily Rundown\n",
            "IAShowID  :  MSNBCW_20140319_130000_The_Daily_Rundown\n",
            "IAPreviewThumb  :  https://archive.org/download/MSNBCW_20140319_130000_The_Daily_Rundown/MSNBCW_20140319_130000_The_Daily_Rundown.thumbs/MSNBCW_20140319_130000_The_Daily_Rundown_002926.jpg\n",
            "Snippet  :  other tools to help communities to become more prepared and resilient toward climate change. it's calling on corporate innovative giants like google and microsoft and intel to use this federal data to develop new tools. ultimately, this gives the academic community, essentially, more access to federal\n",
            "Document : /content/drive/My Drive/AIR/TelevisionNews/MSNBC.201610.csv\n",
            "URL  :  https://archive.org/details/MSNBCW_20161001_110000_The_Place_for_Politics_2016#start/615/end/650\n",
            "MatchDateTime  :  10/1/2016 11:10:30\n",
            "Station  :  MSNBC\n",
            "Show  :  The Place for Politics 2016\n",
            "IAShowID  :  MSNBCW_20161001_110000_The_Place_for_Politics_2016\n",
            "IAPreviewThumb  :  https://archive.org/download/MSNBCW_20161001_110000_The_Place_for_Politics_2016/MSNBCW_20161001_110000_The_Place_for_Politics_2016.thumbs/MSNBCW_20161001_110000_The_Place_for_Politics_2016_000598.jpg\n",
            "Snippet  :  it wasn't his best look. i think we saw him repeated deleon eye things that you could -- deny things that you could easily google. his most re-tweeted remark was how he said climate change is a hoax perpetrated by the chinese. an old tweet for sure, but\n",
            "Document : /content/drive/My Drive/AIR/TelevisionNews/MSNBC.201504.csv\n",
            "URL  :  https://archive.org/details/MSNBCW_20150407_210000_The_Ed_Show#start/2341/end/2376\n",
            "MatchDateTime  :  4/7/2015 21:39:16\n",
            "Station  :  MSNBC\n",
            "Show  :  The Ed Show\n",
            "IAShowID  :  MSNBCW_20150407_210000_The_Ed_Show\n",
            "IAPreviewThumb  :  https://archive.org/download/MSNBCW_20150407_210000_The_Ed_Show/MSNBCW_20150407_210000_The_Ed_Show.thumbs/MSNBCW_20150407_210000_The_Ed_Show_002325.jpg\n",
            "Snippet  :  ties with alec. the ceo of google said alec was lying about climate change and left the group back in september. republicans are following the money. conservative politicians won't enact climate saving policies but they're trying to opt out of\n",
            "Document : /content/drive/My Drive/AIR/TelevisionNews/MSNBC.201904.csv\n",
            "URL  :  https://archive.org/details/MSNBCW_20190409_190000_MSNBC_Live_With_Ali_Velshi#start/95/end/130\n",
            "MatchDateTime  :  4/9/2019 19:01:50\n",
            "Station  :  MSNBC\n",
            "Show  :  MSNBC Live With Ali Velshi\n",
            "IAShowID  :  MSNBCW_20190409_190000_MSNBC_Live_With_Ali_Velshi\n",
            "IAPreviewThumb  :  https://archive.org/download/MSNBCW_20190409_190000_MSNBC_Live_With_Ali_Velshi/MSNBCW_20190409_190000_MSNBC_Live_With_Ali_Velshi.thumbs/MSNBCW_20190409_190000_MSNBC_Live_With_Ali_Velshi_000089.jpg\n",
            "Snippet  :  epa's budget cuts defending the white house's proposal to slash funding by over 30% and other top officials from the obama administration also testified on climate change, we had tech executives from google and facebook testifying on the\n",
            "Document : /content/drive/My Drive/AIR/TelevisionNews/MSNBC.201708.csv\n",
            "URL  :  https://archive.org/details/MSNBCW_20170820_160000_MSNBC_Live_With_Alex_Witt#start/2035/end/2070\n",
            "MatchDateTime  :  8/20/2017 16:34:10\n",
            "Station  :  MSNBC\n",
            "Show  :  MSNBC Live With Alex Witt\n",
            "IAShowID  :  MSNBCW_20170820_160000_MSNBC_Live_With_Alex_Witt\n",
            "IAPreviewThumb  :  https://archive.org/download/MSNBCW_20170820_160000_MSNBC_Live_With_Alex_Witt/MSNBCW_20170820_160000_MSNBC_Live_With_Alex_Witt.thumbs/MSNBCW_20170820_160000_MSNBC_Live_With_Alex_Witt_002008.jpg\n",
            "Snippet  :  giving them kombucha shakes at google to walking the talk on a lot of social issues. not just social issues, immigration, climate change and this. it's not -- i don't want to say it's a final straw, but it is. it's enough.\n",
            "Document : /content/drive/My Drive/AIR/TelevisionNews/MSNBC.201608.csv\n",
            "URL  :  https://archive.org/details/MSNBCW_20160827_093000_Your_Business#start/1688/end/1723\n",
            "MatchDateTime  :  8/27/2016 9:58:23\n",
            "Station  :  MSNBC\n",
            "Show  :  Your Business\n",
            "IAShowID  :  MSNBCW_20160827_093000_Your_Business\n",
            "IAPreviewThumb  :  https://archive.org/download/MSNBCW_20160827_093000_Your_Business/MSNBCW_20160827_093000_Your_Business.thumbs/MSNBCW_20160827_093000_Your_Business_001677.jpg\n",
            "Snippet  :  climate change. i get a lot of that information through google alert. i get a full feed of information on a constant base i which provides me with a lot of information for my blogs. i run two blogs for tom wood\n",
            "Document : /content/drive/My Drive/AIR/TelevisionNews/MSNBC.201608.csv\n",
            "URL  :  https://archive.org/details/MSNBCW_20160821_113000_Your_Business#start/1677/end/1712\n",
            "MatchDateTime  :  8/21/2016 11:58:12\n",
            "Station  :  MSNBC\n",
            "Show  :  Your Business\n",
            "IAShowID  :  MSNBCW_20160821_113000_Your_Business\n",
            "IAPreviewThumb  :  https://archive.org/download/MSNBCW_20160821_113000_Your_Business/MSNBCW_20160821_113000_Your_Business.thumbs/MSNBCW_20160821_113000_Your_Business_001677.jpg\n",
            "Snippet  :  one of the tools that i use that is really helpful is google alerts. there's a lot of research and stories that are done and presented about maple and the industry and sustainability and climate change and i get that through google alert so i get\n",
            "Document : /content/drive/My Drive/AIR/TelevisionNews/MSNBC.201909.csv\n",
            "URL  :  https://archive.org/details/MSNBCW_20190918_130000_MSNBC_Live_With_Stephanie_Ruhle#start/1452/end/1487\n",
            "MatchDateTime  :  9/18/2019 13:24:27\n",
            "Station  :  MSNBC\n",
            "Show  :  MSNBC Live With Stephanie Ruhle\n",
            "IAShowID  :  MSNBCW_20190918_130000_MSNBC_Live_With_Stephanie_Ruhle\n",
            "IAPreviewThumb  :  https://archive.org/download/MSNBCW_20190918_130000_MSNBC_Live_With_Stephanie_Ruhle/MSNBCW_20190918_130000_MSNBC_Live_With_Stephanie_Ruhle.thumbs/MSNBCW_20190918_130000_MSNBC_Live_With_Stephanie_Ruhle_001438.jpg\n",
            "Snippet  :  policymakers to combat climate change before the u.n. climate summit new york city has already announced that public school students can skip classes without penalties. and several major technology companies, including google, amazon and microsoft, have announced that their employees will be walking out in support\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYuLzYXlubS7",
        "outputId": "01d7e926-bd2e-4956-aaa7-4e14dcaaad1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "keywords"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6Q-OqATxNnN"
      },
      "source": [
        "Creating Index for ES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgdXRDJKOql6"
      },
      "source": [
        "#es.indices.delete(index=\"news\")\n",
        "es.indices.create(index=\"news\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSiZFbXNO3b6"
      },
      "source": [
        "#uploading files to ELASTIC SEARCH\n",
        "folder=\"/content/drive/My Drive/AIR/TelevisionNews/\"\n",
        "count=1\n",
        "for i in glob.glob(folder+\"/*csv\"):\n",
        "    path=i\n",
        "    try:\n",
        "        df=pd.read_csv(path)\n",
        "        for row in df.index:\n",
        "            data={}\n",
        "            for col in df.columns:\n",
        "                if(len(df[col][row])!=0):\n",
        "                    data[col]=df[col][row]\n",
        "                else:\n",
        "                    data[col]=\"empty\"\n",
        "            data[\"doc\"]=path\n",
        "            data[\"snippet\"]=row\n",
        "            try:\n",
        "                es.index(index=\"news\", doc_type=\"word\", id=count, body=data)\n",
        "            except:\n",
        "                print(path,\" \",row)\n",
        "            count+=1\n",
        "    except:\n",
        "        continue"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZQFAGEDPdZs"
      },
      "source": [
        "#ELASTIC SEARCH\n",
        "body = {\n",
        "    \"from\":0,\n",
        "    \"size\":10000,\n",
        "    \"query\": {\n",
        "        \"match\": {\n",
        "            \"Snippet\":query\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "ground_truth = es.search(index=\"news\", body=body)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-wO5SbpX-Kw"
      },
      "source": [
        "\n",
        "#EVALUATION METRIC\n",
        "tp=0\n",
        "for i in ground_truth['hits']['hits']:\n",
        "  for j in results: \n",
        "    if(i['_source']['doc']==j[0] and i['_source']['snippet']==j[1]):\n",
        "      tp+=1\n",
        "      break\n",
        "fp=len(results)-tp\n",
        "fn=len(ground_truth['hits']['hits'])-tp\n",
        "precision=tp/(tp+fp)\n",
        "recall=tp/(tp+fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIX1obs3JppY"
      },
      "source": [
        "#MAP\n",
        "avg_precision=[]\n",
        "num=0\n",
        "den=0\n",
        "for i in results:\n",
        "  found=False\n",
        "  for i in ground_truth['hits']['hits']:\n",
        "    if(i['_source']['doc']==j[0] and i['_source']['snippet']==j[1]):\n",
        "      num+=1\n",
        "      den+=1\n",
        "      avg_precision.append(num/den)\n",
        "      found=True\n",
        "      break\n",
        "    if(found==False):\n",
        "      den+=1\n",
        "      avg_precision.append(num/den)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}